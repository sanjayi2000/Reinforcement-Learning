{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the policy network\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, action_dim)\n",
    "        self.fc_std = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        mean = torch.tanh(self.fc2(x))  # Action mean\n",
    "        std = torch.exp(self.fc_std(x))  # Action std (positive)\n",
    "        return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_gradient(env, policy_net, optimizer, num_episodes=1000, gamma=0.99, render=False):\n",
    "    \n",
    "    rewards_history = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        state = np.array(state, dtype=np.float32)\n",
    "\n",
    "        log_probs = []\n",
    "        rewards = []\n",
    "\n",
    "        done = False\n",
    "        while not done:\n",
    "            if render and episode % 100 == 0:\n",
    "                env.render()\n",
    "\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "            mean, std = policy_net(state_tensor)\n",
    "            dist = Normal(mean, std)\n",
    "            action = dist.sample()\n",
    "            action = action.clamp(env.action_space.low[0], env.action_space.high[0])\n",
    "\n",
    "            next_state, reward, done, _, _ = env.step(action.detach().numpy())\n",
    "            next_state = np.array(next_state, dtype=np.float32)\n",
    "\n",
    "            log_probs.append(dist.log_prob(action).sum())\n",
    "            rewards.append(reward)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        # Compute discounted rewards\n",
    "        discounted_rewards = []\n",
    "        G = 0\n",
    "        for reward in reversed(rewards):\n",
    "            G = reward + gamma * G\n",
    "            discounted_rewards.insert(0, G)\n",
    "        discounted_rewards = torch.tensor(discounted_rewards, dtype=torch.float32)\n",
    "        discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-8)\n",
    "\n",
    "        # Compute policy loss\n",
    "        policy_loss = []\n",
    "        for log_prob, G in zip(log_probs, discounted_rewards):\n",
    "            policy_loss.append(-log_prob * G)\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "\n",
    "        # Update policy network\n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track and log progress\n",
    "        total_reward = sum(rewards)\n",
    "        rewards_history.append(total_reward)\n",
    "\n",
    "        if episode % 100 == 0:\n",
    "            print(f\"Episode {episode}: Total Reward: {total_reward}\")\n",
    "\n",
    "    # Plot the reward progression\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(rewards_history, label=\"Total Reward per Episode\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Total Reward\")\n",
    "    plt.title(\"Reward Progression\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import gym\n",
    "\n",
    "    env = gym.make(\"MountainCarContinuous-v0\", render_mode=\"human\")\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "\n",
    "    policy_net = PolicyNetwork(state_dim, action_dim)\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=1e-3)\n",
    "\n",
    "    # Enable rendering and plotting\n",
    "    policy_gradient(env, policy_net, optimizer, render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project implemented a policy gradient method for continuous action spaces using a reinforcement learning framework. The model was trained on the MountainCarContinuous-v0 environment, where it demonstrated the ability to learn effective control strategies through policy optimization.\n",
    "\n",
    "Key Results:\n",
    "\n",
    "    Performance Improvement: Over successive training episodes, the agent's cumulative rewards showed consistent improvement, as visualized in the reward progression graph.\n",
    "    Inference Behavior: During the evaluation phase, the trained policy consistently navigated the environment effectively, reaching the goal state with higher rewards compared to random or untrained policies.\n",
    "    Visualization: Rendering the agent's behavior during inference highlighted its ability to make smooth and goal-oriented actions, indicative of a well-learned policy.\n",
    "\n",
    "These results underscore the potential of policy gradient methods in solving complex control problems, paving the way for further exploration with more advanced reinforcement learning techniques."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
